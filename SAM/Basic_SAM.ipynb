{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'patchify'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpatchify\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m patchify\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'patchify'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models \n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from patchify import patchify\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \"./qatacov19-dataset\" (use force=True to force download)\n"
     ]
    }
   ],
   "source": [
    "#Download Dataset\n",
    "import opendatasets as od\n",
    "\n",
    "od.download(\"https://www.kaggle.com/datasets/aysendegerli/qatacov19-dataset/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python                                                  \n",
    "from PIL import Image                                              \n",
    "import os, sys                       \n",
    "\n",
    "train_img_path = \"/home/cahsi/Josh/SAM/work/SAM/qatacov19-dataset/QaTa-COV19/QaTa-COV19-v2/Train Set/Images/*.png\"\n",
    "test_img_path = \"/home/cahsi/Josh/SAM/work/SAM/qatacov19-dataset/QaTa-COV19/QaTa-COV19-v2/Test Set/Images/*.png\"\n",
    "\n",
    "train_mask_path = \"/home/cahsi/Josh/SAM/work/SAM/qatacov19-dataset/QaTa-COV19/QaTa-COV19-v2/Train Set/Ground-truths/*.png\"\n",
    "test_mask_path = \"/home/cahsi/Josh/SAM/work/SAM/qatacov19-dataset/QaTa-COV19/QaTa-COV19-v2/Test Set/Ground-truths/*.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "\n",
    "# Load all images in the current folder that end with .png\n",
    "train_img = io.imread_collection(train_img_path)\n",
    "test_img = io.imread_collection(test_img_path)\n",
    "train_mask = io.imread_collection(train_mask_path)\n",
    "test_mask = io.imread_collection(test_mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training raw images: 7145      Shape of an training raw image: (224, 224)\n",
      "Length of training mask images: 2113     Shape of an training mask image: (224, 224)\n",
      "\n",
      "Length of test raw images: 7145       Shape of an test raw image: (224, 224)\n",
      "Length of test mask images: 2113      Shape of an test mask image: (224, 224)\n"
     ]
    }
   ],
   "source": [
    "#Images Info\n",
    "print(\"Length of training raw images: \" + str(len(train_images_raw)) + \"      Shape of an training raw image: \" + str(train_images_raw[0].shape))\n",
    "print(\"Length of training mask images: \" + str(len(train_images_masks)) + \"     Shape of an training mask image: \" + str(train_images_masks[0].shape))\n",
    "\n",
    "print(\"\\nLength of test raw images: \" + str(len(test_images_raw)) + \"       Shape of an test raw image: \" + str(test_images_raw[0].shape))\n",
    "print(\"Length of test mask images: \" + str(len(test_images_masks)) + \"      Shape of an test mask image: \" + str(test_images_masks[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resizing Images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "def resize_images(images, mask):\n",
    "    output = []\n",
    "    if mask:\n",
    "        for mask in images:\n",
    "            # Perform resizing with nearest neighbor interpolation to maintain binary values\n",
    "            resized_mask = (resize(mask, (256, 256), order=0, anti_aliasing=False) > 0.5).astype(np.uint8)\n",
    "            output.append(resized_mask)\n",
    "    else:\n",
    "        for image in images:\n",
    "            resized_image = resize(image, (256, 256), anti_aliasing=False)\n",
    "            output.append(resized_image)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Resize training images to 256x256\n",
    "train_img = resize_images(train_img, False)\n",
    "\n",
    "# Resize testing images to 256x256\n",
    "test_img = resize_images(test_img, False)\n",
    "\n",
    "# Resize training masks to 256x256\n",
    "train_mask = resize_images(train_mask, True)\n",
    "\n",
    "# Resize testing masks to 256x256\n",
    "test_mask = resize_images(test_mask, True)\n",
    "\n",
    "#Convert to np array\n",
    "train_img = np.array(train_img)\n",
    "test_img = np.array(test_img)\n",
    "train_mask = np.array(train_mask)\n",
    "test_mask = np.array(test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of resized training raw image: (7145, 256, 256)\n",
      "Shape of resized testing raw image: (7145, 256, 256)\n",
      "Shape of resized training mask image: (2113, 256, 256)\n",
      "Shape of resized testing mask image: (2113, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "#Print Shape of resized images\n",
    "print(\"Shape of resized training raw image: \" + str(train_images.shape))\n",
    "print(\"Shape of resized testing raw image: \" + str(test_images.shape))\n",
    "print(\"Shape of resized training mask image: \" + str(train_masks.shape))\n",
    "print(\"Shape of resized testing mask image: \" + str(test_masks.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of resized images: (14290, 256, 256)\n",
      "Shape of resized masks: (4226, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "#Combine images and masks respectively\n",
    "images = np.concatenate((train_images, test_images))\n",
    "masks = np.concatenate((train_masks, test_masks))\n",
    "\n",
    "print(\"Shape of resized images: \" + str(images.shape))\n",
    "print(\"Shape of resized masks: \" + str(masks.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "Column 1 named mask expected length 14290 but got length 4226",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 11\u001b[0m\n\u001b[1;32m      5\u001b[0m dataset_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: [Image\u001b[38;5;241m.\u001b[39mfromarray(img) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images],\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m: [Image\u001b[38;5;241m.\u001b[39mfromarray(mask) \u001b[38;5;28;01mfor\u001b[39;00m mask \u001b[38;5;129;01min\u001b[39;00m masks],\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Create the dataset using the datasets.Dataset class\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Josh/SAM/work/lib/python3.9/site-packages/datasets/arrow_dataset.py:912\u001b[0m, in \u001b[0;36mDataset.from_dict\u001b[0;34m(cls, mapping, features, info, split)\u001b[0m\n\u001b[1;32m    910\u001b[0m     arrow_typed_mapping[col] \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    911\u001b[0m mapping \u001b[38;5;241m=\u001b[39m arrow_typed_mapping\n\u001b[0;32m--> 912\u001b[0m pa_table \u001b[38;5;241m=\u001b[39m \u001b[43mInMemoryTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pydict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m     info \u001b[38;5;241m=\u001b[39m DatasetInfo()\n",
      "File \u001b[0;32m~/Josh/SAM/work/lib/python3.9/site-packages/datasets/table.py:758\u001b[0m, in \u001b[0;36mInMemoryTable.from_pydict\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pydict\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    744\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;124;03m    Construct a Table from Arrow arrays or columns.\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;124;03m        `datasets.table.Table`\u001b[39;00m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 758\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pydict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Josh/SAM/work/lib/python3.9/site-packages/pyarrow/table.pxi:1813\u001b[0m, in \u001b[0;36mpyarrow.lib._Tabular.from_pydict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Josh/SAM/work/lib/python3.9/site-packages/pyarrow/table.pxi:5348\u001b[0m, in \u001b[0;36mpyarrow.lib._from_pydict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Josh/SAM/work/lib/python3.9/site-packages/pyarrow/table.pxi:3991\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_arrays\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Josh/SAM/work/lib/python3.9/site-packages/pyarrow/table.pxi:3271\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.validate\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Josh/SAM/work/lib/python3.9/site-packages/pyarrow/error.pxi:91\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: Column 1 named mask expected length 14290 but got length 4226"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "# Convert the NumPy arrays to Pillow images and store them in a dictionary\n",
    "dataset_dict = {\n",
    "    \"image\": [Image.fromarray(img) for img in images],\n",
    "    \"mask\": [Image.fromarray(mask) for mask in masks],\n",
    "}\n",
    "\n",
    "# Create the dataset using the datasets.Dataset class\n",
    "dataset = Dataset.from_dict(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check dataset info\n",
    "print(\"Dataset Info:\\n\" + str(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get random image from training set and plot\n",
    "img_num = random.randint(0, images.shape[0]-1)\n",
    "example_image = dataset[img_num][\"image\"]\n",
    "example_mask = dataset[img_num][\"mask\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Plot the first image on the left\n",
    "axes[0].imshow(np.array(example_image), cmap='gray')  # Assuming the first image is grayscale\n",
    "axes[0].set_title(\"Image\")\n",
    "\n",
    "# Plot the second image on the right\n",
    "axes[1].imshow(example_mask, cmap='gray')  # Assuming the second image is grayscale\n",
    "axes[1].set_title(\"Mask\")\n",
    "\n",
    "# Hide axis ticks and labels\n",
    "for ax in axes:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "# Display the images side by side\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that gets bounding boxes from masks\n",
    "def get_bounding_box(ground_truth_map):\n",
    "  # get bounding box from mask\n",
    "  y_indices, x_indices = np.where(ground_truth_map > 0)\n",
    "  x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "  y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "  # add perturbation to bounding box coordinates\n",
    "  H, W = ground_truth_map.shape\n",
    "  x_min = max(0, x_min - np.random.randint(0, 20))\n",
    "  x_max = min(W, x_max + np.random.randint(0, 20))\n",
    "  y_min = max(0, y_min - np.random.randint(0, 20))\n",
    "  y_max = min(H, y_max + np.random.randint(0, 20))\n",
    "  bbox = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "  return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SAMDataset(Dataset):\n",
    "  \"\"\"\n",
    "  This class is used to create a dataset that serves input images and masks.\n",
    "  It takes a dataset and a processor as input and overrides the __len__ and __getitem__ methods of the Dataset class.\n",
    "  \"\"\"\n",
    "  def __init__(self, dataset, processor):\n",
    "    self.dataset = dataset\n",
    "    self.processor = processor\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    item = self.dataset[idx]\n",
    "    image = item[\"image\"]\n",
    "    ground_truth_mask = np.array(item[\"mask\"])\n",
    "\n",
    "    image = np.array(image.convert(\"RGB\"))\n",
    "\n",
    "    # get bounding box prompt\n",
    "    prompt = get_bounding_box(ground_truth_mask)\n",
    "\n",
    "    # prepare image and prompt for the model\n",
    "    inputs = self.processor(image, input_boxes=[[prompt]], return_tensors=\"pt\")\n",
    "\n",
    "    # remove batch dimension which the processor adds by default\n",
    "    inputs = {k:v.squeeze(0) for k,v in inputs.items()}\n",
    "\n",
    "    # add ground truth segmentation\n",
    "    inputs[\"ground_truth_mask\"] = ground_truth_mask\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the processor\n",
    "from transformers import SamProcessor\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the SAMDataset\n",
    "train_dataset = SAMDataset(dataset=dataset, processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training: \\n\")\n",
    "example = train_dataset[0]\n",
    "for k,v in example.items():\n",
    "  print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader instance for the training dataset\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training: \\n\")\n",
    "train_batch = next(iter(train_dataloader))\n",
    "for k,v in train_batch.items():\n",
    "  print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Batch Shape: \" + str(train_batch[\"ground_truth_mask\"].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "from transformers import SamModel\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "# make sure we only compute gradients for mask decoder\n",
    "for name, param in model.named_parameters():\n",
    "  if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import monai\n",
    "# Initialize the optimizer and the loss function\n",
    "optimizer = Adam(model.mask_decoder.parameters(), lr=1e-5, weight_decay=0)\n",
    "#Try DiceFocalLoss, FocalLoss, DiceCELoss\n",
    "seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DataParallel\n",
    "from torch.nn.functional import threshold, normalize\n",
    "\n",
    "#Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "# Move your model to GPU devices\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda:1\")\n",
    "model = model.to(device)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # forward pass\n",
    "        outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
    "                        input_boxes=batch[\"input_boxes\"].to(device),\n",
    "                        multimask_output=False)\n",
    "\n",
    "        # compute loss\n",
    "        predicted_masks = outputs.pred_masks.squeeze(1)\n",
    "        ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
    "        loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))\n",
    "\n",
    "        # backward pass (compute gradients of parameters w.r.t. loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "    print(f'EPOCH: {epoch}')\n",
    "    print(f'Mean loss: {mean(epoch_losses)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state dictionary to a file\n",
    "torch.save(model.state_dict(), \"/home/cahsi/Josh/SAM/sam_model_checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SamModel, SamConfig, SamProcessor\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model configuration\n",
    "model_config = SamConfig.from_pretrained(\"facebook/sam-vit-base\")\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "# Create an instance of the model architecture with the loaded configuration\n",
    "basic_sam_model = SamModel(config=model_config)\n",
    "#Update the model by loading the weights from saved file.\n",
    "basic_sam_model.load_state_dict(torch.load(\"/home/cahsi/Josh/SAM/basic_sam_model_checkpoint.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the device to cuda if available, otherwise use cpu\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "basic_sam_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# let's take a random training example\n",
    "idx = random.randint(0, images.shape[0]-1)\n",
    "\n",
    "# load image\n",
    "test_image = dataset[idx][\"image\"]\n",
    "test_image = np.array(test_image.convert(\"RGB\"))\n",
    "\n",
    "\n",
    "# get box prompt based on ground truth segmentation map\n",
    "ground_truth_mask = np.array(dataset[idx][\"mask\"])\n",
    "prompt = get_bounding_box(ground_truth_mask)\n",
    "\n",
    "# prepare image + box prompt for the model\n",
    "inputs = processor(test_image, input_boxes=[[prompt]], return_tensors=\"pt\")\n",
    "\n",
    "# Move the input tensor to the GPU if it's not already there\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "basic_sam_model.eval()\n",
    "\n",
    "# forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = basic_sam_model(**inputs, multimask_output=False)\n",
    "\n",
    "# apply sigmoid\n",
    "medsam_seg_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
    "# convert soft mask to hard mask\n",
    "medsam_seg_prob = medsam_seg_prob.cpu().numpy().squeeze()\n",
    "medsam_seg = (medsam_seg_prob > 0.5).astype(np.uint8)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot the first image on the left\n",
    "test_image = dataset[idx][\"image\"]\n",
    "axes[0].imshow(np.array(test_image), cmap='gray')  # Assuming the first image is grayscale\n",
    "axes[0].set_title(\"Image\")\n",
    "\n",
    "# Plot the second image on the right\n",
    "axes[1].imshow(medsam_seg, cmap='gray')  # Assuming the second image is grayscale\n",
    "axes[1].set_title(\"Mask\")\n",
    "\n",
    "# Plot the second image on the right\n",
    "axes[2].imshow(medsam_seg_prob)  # Assuming the second image is grayscale\n",
    "axes[2].set_title(\"Probability Map\")\n",
    "\n",
    "# Hide axis ticks and labels\n",
    "for ax in axes:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "# Display the images side by side\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Resources\n",
    "<b> https://github.com/hitachinsk/SAMed\n",
    "\n",
    "<b> https://github.com/MathieuNlp/Sam_LoRA\n",
    "\n",
    "<b>https://colab.research.google.com/github/bnsreenu/python_for_microscopists/blob/master/331_fine_tune_SAM_mito.ipynb#scrollTo=aTXUX7xyCEGT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
