{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models \n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNCOMMENT IF YOU NEED TO DOWNLOAD DATASET------------------------------------------------------------------------------------------------------\n",
    "#Download Dataset\n",
    "# import opendatasets as od\n",
    "\n",
    "# od.download(\"https://www.kaggle.com/datasets/aysendegerli/qatacov19-dataset/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "\n",
    "train_img_path = \"/home/cahsi/Josh/Semantic_Segmentation_Research/LoRA_SAM/qatacov19-dataset/QaTa-COV19/QaTa-COV19-v2/Train Set/Images/*.png\"\n",
    "test_img_path = \"/home/cahsi/Josh/Semantic_Segmentation_Research/LoRA_SAM/qatacov19-dataset/QaTa-COV19/QaTa-COV19-v2/Test Set/Images/*.png\"\n",
    "\n",
    "train_mask_path = \"/home/cahsi/Josh/Semantic_Segmentation_Research/LoRA_SAM/qatacov19-dataset/QaTa-COV19/QaTa-COV19-v2/Train Set/Ground-truths/*.png\"\n",
    "test_mask_path = \"/home/cahsi/Josh/Semantic_Segmentation_Research/LoRA_SAM/qatacov19-dataset/QaTa-COV19/QaTa-COV19-v2/Test Set/Ground-truths/*.png\"\n",
    "\n",
    "# Load all images in the current folder that end with .png\n",
    "train_img = io.imread_collection(train_img_path)\n",
    "test_img = io.imread_collection(test_img_path)\n",
    "train_mask = io.imread_collection(train_mask_path)\n",
    "test_mask = io.imread_collection(test_mask_path)\n",
    "\n",
    "#Images Info\n",
    "print(\"Length of training raw images: \" + str(len(train_img)) + \"      Shape of an training raw image: \" + str(train_img[0].shape))\n",
    "print(\"Length of training mask images: \" + str(len(train_mask)) + \"     Shape of an training mask image: \" + str(train_mask[0].shape))\n",
    "\n",
    "print(\"\\nLength of test raw images: \" + str(len(test_img)) + \"       Shape of an test raw image: \" + str(test_img[0].shape))\n",
    "print(\"Length of test mask images: \" + str(len(test_mask)) + \"      Shape of an test mask image: \" + str(test_mask[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resizing Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "import cv2\n",
    "\n",
    "def resize_images(images, mask):\n",
    "    output = []\n",
    "    if mask:\n",
    "        for mask in images:\n",
    "            # Perform resizing with nearest neighbor interpolation to maintain binary values\n",
    "            resized_mask = (resize(mask, (256, 256), order=0, anti_aliasing=False) > 0.5).astype(np.uint8)\n",
    "            output.append(resized_mask)\n",
    "    else:\n",
    "        for image in images:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "            resized_image = resize(image, (256, 256), anti_aliasing=False)\n",
    "            output.append(resized_image)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Resize training images to 256x256\n",
    "train_images = np.array(resize_images(train_img, False))\n",
    "\n",
    "# Resize testing images to 256x256\n",
    "test_img = np.array(resize_images(test_img, False))\n",
    "\n",
    "# Resize training masks to 256x256\n",
    "train_masks = np.array(resize_images(train_mask, True))\n",
    "\n",
    "# Resize testing masks to 256x256\n",
    "test_mask = np.array(resize_images(test_mask, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print Shape of resized images\n",
    "print(\"Shape of resized training raw image: \" + str(train_images.shape))\n",
    "print(\"Shape of resized training mask image: \" + str(train_masks.shape))\n",
    "print(\"Shape of resized testing raw image: \" + str(test_img.shape))\n",
    "print(\"Shape of resized testing mask image: \" + str(test_mask.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Subset of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Convert to subset of data\n",
    "subset_size = 0.04\n",
    "\n",
    "#>>>>> Comment if you want to use the full dataset for training\n",
    "#train_images, _, train_masks, _ = train_test_split(train_images, train_masks, train_size=subset_size, random_state=25)\n",
    "\n",
    "images = train_images\n",
    "masks = train_masks\n",
    "\n",
    "#Print Shape of resized images\n",
    "print(\"Shape of resized training raw image: \" + str(images.shape))\n",
    "print(\"Shape of resized training mask image: \" + str(masks.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "img = [Image.fromarray((img * 255).astype(np.uint8)) for img in images]\n",
    "label = [Image.fromarray(mask) for mask in masks]\n",
    "\n",
    "# Create Validation Dataset\n",
    "#X_train, X_val, y_train, y_val = train_test_split(img, label, test_size=0.1, random_state=42)\n",
    "\n",
    "# Convert the NumPy arrays to Pillow images and store them in a dictionary\n",
    "training_dataset_dict = {\n",
    "    \"image\": img,\n",
    "    \"mask\": label,\n",
    "}\n",
    "\n",
    "# Convert the NumPy arrays to Pillow images and store them in a dictionary\n",
    "# val_dataset_dict = {\n",
    "#     \"image\": X_val,\n",
    "#     \"mask\": y_val,\n",
    "# }\n",
    "\n",
    "training_dataset = Dataset.from_dict(training_dataset_dict)\n",
    "\n",
    "# val_dataset = Dataset.from_dict(val_dataset_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check dataset info\n",
    "print(\"Train Dataset Info:\\n\" + str(training_dataset))\n",
    "\n",
    "#print(\"Validation Dataset Info:\\n\" + str(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get random image from training set and plot\n",
    "img_num = random.randint(0, images.shape[0]-1)\n",
    "example_image = training_dataset[img_num][\"image\"]\n",
    "example_mask = training_dataset[img_num][\"mask\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 10))\n",
    "\n",
    "# Plot the first image on the left\n",
    "axes[0].imshow(np.array(example_image), cmap='gray')  # Assuming the first image is grayscale\n",
    "axes[0].set_title(\"Image\")\n",
    "\n",
    "# Plot the second image in the middle\n",
    "axes[1].imshow(np.array(example_mask), cmap='gray')  # Assuming the second image is grayscale\n",
    "axes[1].set_title(\"Ground Truth Mask\")\n",
    "\n",
    "# Plot them overlapped on the right\n",
    "axes[2].imshow(np.array(example_image), cmap='gray')  # Assuming the second image is grayscale\n",
    "show_mask(np.array(example_mask), axes[2])\n",
    "axes[2].set_title(\"Result\")\n",
    "\n",
    "\n",
    "# Hide axis ticks and labels\n",
    "for ax in axes:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "# Display the images side by side\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that gets bounding boxes from masks\n",
    "def get_bounding_box(ground_truth_map):\n",
    "  # get bounding box from mask\n",
    "  y_indices, x_indices = np.where(ground_truth_map > 0)\n",
    "  x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "  y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "  # add perturbation to bounding box coordinates\n",
    "  H, W = ground_truth_map.shape\n",
    "  x_min = max(0, x_min - np.random.randint(0, 20))\n",
    "  x_max = min(W, x_max + np.random.randint(0, 20))\n",
    "  y_min = max(0, y_min - np.random.randint(0, 20))\n",
    "  y_max = min(H, y_max + np.random.randint(0, 20))\n",
    "  bbox = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "  return bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAM Dataset/Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SAMDataset(Dataset):\n",
    "  \"\"\"\n",
    "  This class is used to create a dataset that serves input images and masks.\n",
    "  It takes a dataset and a processor as input and overrides the __len__ and __getitem__ methods of the Dataset class.\n",
    "  \"\"\"\n",
    "  def __init__(self, dataset, processor):\n",
    "    self.dataset = dataset\n",
    "    self.processor = processor\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    item = self.dataset[idx]\n",
    "    image = item[\"image\"]\n",
    "    ground_truth_mask = np.array(item[\"mask\"])\n",
    "\n",
    "    # get bounding box prompt\n",
    "    prompt = get_bounding_box(ground_truth_mask)\n",
    "\n",
    "    # prepare image and prompt for the model\n",
    "    inputs = self.processor(image, input_boxes=[[prompt]], return_tensors=\"pt\")\n",
    "\n",
    "    # remove batch dimension which the processor adds by default\n",
    "    inputs = {k:v.squeeze(0) for k,v in inputs.items()}\n",
    "\n",
    "    # add ground truth segmentation\n",
    "    inputs[\"ground_truth_mask\"] = ground_truth_mask\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the processor\n",
    "from transformers import SamProcessor\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "# Create an instance of the SAMDataset\n",
    "train_dataset = SAMDataset(dataset=training_dataset, processor=processor)\n",
    "\n",
    "# # Create an instance of the SAMDataset\n",
    "# val_dataset = SAMDataset(dataset=val_dataset, processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader instance for the training/validation dataset\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, drop_last=False)\n",
    "#val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=True, drop_last=False)\n",
    "\n",
    "print(\"Training: \\n\")\n",
    "train_batch = next(iter(train_dataloader))\n",
    "for k,v in train_batch.items():\n",
    "  print(k,v.shape)\n",
    "\n",
    "print(\"Ground Truth Mask Shape: \" + str(train_batch[\"ground_truth_mask\"].shape))\n",
    "\n",
    "# print(\"Validation: \\n\")\n",
    "# val_batch = next(iter(val_dataloader))\n",
    "# for k,v in val_batch.items():\n",
    "#   print(k,v.shape)\n",
    "\n",
    "#print(\"Ground Truth Mask Shape: \" + str(val_batch[\"ground_truth_mask\"].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SAM model\n",
    "from transformers import SamModel\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "# make sure we only compute gradients for vision encoder\n",
    "for name, param in model.named_parameters():\n",
    "  if name.startswith(\"prompt_encoder\"):# or name.startswith(\"vision_encoder\"):\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sam_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loralib as lora\n",
    "\n",
    "rank = 2\n",
    "\n",
    "# VISION ENCODER -----------------------------------------------------------------------------------------------------------\n",
    "# # Patch embedding\n",
    "# model.vision_encoder.patch_embed.projection = lora.Conv2d(3, 768, kernel_size=16, stride=(16, 16), r = rank)\n",
    "\n",
    "for layer in model.vision_encoder.layers:\n",
    "    #Attention block\n",
    "    layer.attn.qkv = lora.MergedLinear(768, 2304, r=rank, enable_lora=[True, True, True])\n",
    "    # layer.attn.proj = lora.Linear(768, 768, r=rank)\n",
    "    \n",
    "#     # #MLP block\n",
    "#     layer.mlp.lin1 = lora.Linear(768, 3072, r=rank)\n",
    "#     layer.mlp.lin2 = lora.Linear(3072, 768, r=rank)\n",
    "\n",
    "# #Neck Layer\n",
    "# model.vision_encoder.neck.conv1 = lora.Conv2d(768, 256, kernel_size=1, stride=(1, 1), r=rank)\n",
    "# model.vision_encoder.neck.conv2 = lora.Conv2d(256, 256, kernel_size=3, stride=(1, 1), padding=(1, 1), r=rank)\n",
    "\n",
    "\n",
    "# MASK DECODER -----------------------------------------------------------------------------------------------------------\n",
    "for layer in model.mask_decoder.transformer.layers:\n",
    "    #Self attention block\n",
    "    layer.self_attn.q_proj = lora.Linear(256, 256, r=rank)\n",
    "    layer.self_attn.k_proj = lora.Linear(256, 256, r=rank)\n",
    "    layer.self_attn.v_proj = lora.Linear(256, 256, r=rank)\n",
    "#     layer.self_attn.out_proj = lora.Linear(256, 256, r=rank)\n",
    "\n",
    "#     #Cross attention block (Token -> Image)\n",
    "#     layer.cross_attn_token_to_image.q_proj = lora.Linear(256, 128, r=rank)\n",
    "#     layer.cross_attn_token_to_image.k_proj = lora.Linear(256, 128, r=rank)\n",
    "#     layer.cross_attn_token_to_image.v_proj = lora.Linear(256, 128, r=rank)\n",
    "#     layer.cross_attn_token_to_image.out_proj = lora.Linear(128, 256, r=rank)\n",
    "\n",
    "#     #MLP block\n",
    "#     layer.mlp.lin1 = lora.Linear(256, 2048, r=rank)\n",
    "#     layer.mlp.lin2 = lora.Linear(2048, 256, r=rank)\n",
    "    \n",
    "#     #Cross attention block (Image -> Token)\n",
    "#     layer.cross_attn_image_to_token.q_proj = lora.Linear(256, 128, r=rank)\n",
    "#     layer.cross_attn_image_to_token.k_proj = lora.Linear(256, 128, r=rank)\n",
    "#     layer.cross_attn_image_to_token.v_proj = lora.Linear(256, 128, r=rank)\n",
    "#     layer.cross_attn_image_to_token.out_proj = lora.Linear(128, 256, r=rank)\n",
    "\n",
    "# #Final attention block (Token -> Image)\n",
    "# model.mask_decoder.transformer.final_attn_token_to_image.q_proj = lora.Linear(256, 128, r=rank)\n",
    "# model.mask_decoder.transformer.final_attn_token_to_image.k_proj = lora.Linear(256, 128, r=rank)\n",
    "# model.mask_decoder.transformer.final_attn_token_to_image.v_proj = lora.Linear(256, 128, r=rank)\n",
    "# model.mask_decoder.transformer.final_attn_token_to_image.out_proj = lora.Linear(128, 256, r=rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original SAM total params: {original_sam_total_params}\")\n",
    "\n",
    "sam_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"LoRA-SAM total params: {sam_total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import monai\n",
    "\n",
    "# Initialize the optimizer and the loss function\n",
    "optimizer = Adam(model.parameters(), lr=1e-5, weight_decay=0)\n",
    "\n",
    "#Try DiceFocalLoss, FocalLoss, DiceCELoss\n",
    "seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DataParallel\n",
    "from torch.nn.functional import threshold, normalize\n",
    "\n",
    "#Training loop\n",
    "num_epochs = 20\n",
    "mean_epoch_loss = []\n",
    "\n",
    "# This sets requires_grad to False for all parameters without the string \"lora_\" in their names\n",
    "#lora.mark_only_lora_as_trainable(model)\n",
    "\n",
    "# #Set layer-norms in VISION ENCODER to be trainable\n",
    "# for layer in model.vision_encoder.layers:\n",
    "#     layer.layer_norm1.requires_grad_(True)\n",
    "#     layer.layer_norm2.requires_grad_(True)\n",
    "# model.vision_encoder.neck.layer_norm1.requires_grad_(True)\n",
    "# model.vision_encoder.neck.layer_norm2.requires_grad_(True)\n",
    "\n",
    "# #Set layer-norms in MASK DECODER to be trainable\n",
    "# for layer in model.mask_decoder.transformer.layers:\n",
    "#     layer.layer_norm1.requires_grad_(True)\n",
    "#     layer.layer_norm2.requires_grad_(True)\n",
    "#     layer.layer_norm3.requires_grad_(True)\n",
    "#     layer.layer_norm4.requires_grad_(True)\n",
    "# model.mask_decoder.transformer.layer_norm_final_attn.requires_grad_(True)\n",
    "\n",
    "\n",
    "# Move your model to GPU devices\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda:7\")\n",
    "model = model.to(device)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # forward pass\n",
    "        outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
    "                        input_boxes=batch[\"input_boxes\"].to(device),\n",
    "                        multimask_output=False)\n",
    "\n",
    "        # compute loss\n",
    "        predicted_masks = outputs.pred_masks.squeeze(1)\n",
    "        ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
    "        loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))\n",
    "\n",
    "        # backward pass (compute gradients of parameters w.r.t. loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "    print(f'EPOCH: {epoch}')\n",
    "    print(f'Mean loss: {mean(epoch_losses)}')\n",
    "    mean_epoch_loss.append(mean_epoch_loss)\n",
    "\n",
    "    # # Validation phase\n",
    "    # model.eval()  # Set model to evaluation mode\n",
    "    # val_losses = []\n",
    "    # with torch.no_grad():  # Disable gradient calculation during validation\n",
    "    #     for batch in tqdm(val_dataloader):\n",
    "    #         # Forward pass\n",
    "    #         outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
    "    #                         input_boxes=batch[\"input_boxes\"].to(device),\n",
    "    #                         multimask_output=False)\n",
    "\n",
    "    #         # Compute loss\n",
    "    #         predicted_masks = outputs.pred_masks.squeeze(1)\n",
    "    #         ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
    "    #         val_loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))\n",
    "    #         val_losses.append(val_loss.item())\n",
    "\n",
    "    # # Print validation loss for the epoch\n",
    "    # print(f'Validation Mean loss: {mean(val_losses)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model's state dictionary to a file\n",
    "# torch.save({\n",
    "#             'epoch': epoch,\n",
    "#             'model_state_dict': model.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "#             'loss': mean(epoch_losses),\n",
    "#             }, f\"/home/cahsi/Josh/Research/venv/Semantic_Segmentation_Research/LoRA_SAM/LoRA_sam_model_checkpoint_rank{rank}.pth\")\n",
    "\n",
    "# # Save the LoRA parameters of the model\n",
    "# torch.save(lora.lora_state_dict(model), f\"/home/cahsi/Josh/Research/venv/Semantic_Segmentation_Research/LoRA_SAM/lora_rank{rank}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the model configuration\n",
    "# model_config = SamConfig.from_pretrained(\"facebook/sam-vit-base\")\n",
    "# processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "# # Create an instance of the model architecture with the loaded configuration\n",
    "# lora_model = SamModel(config=model_config)\n",
    "\n",
    "# #Load pretrained model\n",
    "# lora_model.load_state_dict(torch.load(f\"/home/cahsi/Josh/Research/venv/Semantic_Segmentation_Research/LoRA_SAM/LoRA_sam_model_checkpoint_rank{rank}.pth\"), strict=False)\n",
    "\n",
    "# #Load LoRA checkpoint\n",
    "# lora_model.load_state_dict(torch.load(f\"/home/cahsi/Josh/Research/venv/Semantic_Segmentation_Research/LoRA_SAM/lora_rank{rank}.pt\"), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the device to cuda if available, otherwise use cpu\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lora_model = model\n",
    "lora_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# let's take a random training example\n",
    "idx = random.randint(0, images.shape[0]-1)\n",
    "\n",
    "# load image\n",
    "test_image = training_dataset[idx][\"image\"]\n",
    "test_image = np.array(test_image.convert(\"RGB\"))\n",
    "\n",
    "# get box prompt based on ground truth segmentation map\n",
    "ground_truth_mask = np.array(training_dataset[idx][\"mask\"])\n",
    "prompt = get_bounding_box(ground_truth_mask)\n",
    "\n",
    "# prepare image + box prompt for the model\n",
    "inputs = processor(test_image, input_boxes=[[prompt]], return_tensors=\"pt\")\n",
    "\n",
    "# Move the input tensor to the GPU if it's not already there\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "lora_model.eval()\n",
    "\n",
    "# forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = lora_model(**inputs, multimask_output=False)\n",
    "\n",
    "# apply sigmoid\n",
    "medsam_seg_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
    "# convert soft mask to hard mask\n",
    "medsam_seg_prob = medsam_seg_prob.cpu().numpy().squeeze()\n",
    "medsam_seg = (medsam_seg_prob > 0.5).astype(np.uint8)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 10))\n",
    "\n",
    "# Plot the first image on the left\n",
    "axes[0].imshow(test_image, cmap='gray')  # Assuming the first image is grayscale\n",
    "show_mask(np.array(ground_truth_mask), axes[0])\n",
    "axes[0].set_title(\"Ground Truth Mask\")\n",
    "\n",
    "# Plot the second image on the right\n",
    "axes[1].imshow(test_image, cmap='gray')  # Assuming the second image is grayscale\n",
    "show_mask(np.array(medsam_seg), axes[1])\n",
    "axes[1].set_title(\"Predicted Mask\")\n",
    "\n",
    "# Hide axis ticks and labels\n",
    "for ax in axes:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "# Display the images side by side\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateIoU(ground_mask, pred_mask):\n",
    "        # Calculate the TP, FP, FN\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        FN = 0\n",
    " \n",
    "        for i in range(len(ground_mask)):\n",
    "            for j in range(len(ground_mask[0])):\n",
    "                if ground_mask[i][j] == 1 and pred_mask[i][j] == 1:\n",
    "                    TP += 1\n",
    "                elif ground_mask[i][j] == 0 and pred_mask[i][j] == 1:\n",
    "                    FP += 1\n",
    "                elif ground_mask[i][j] == 1 and pred_mask[i][j] == 0:\n",
    "                    FN += 1\n",
    " \n",
    "        # Calculate IoU\n",
    "        iou = TP / (TP + FP + FN)\n",
    " \n",
    "        return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_ground_mask = np.array(training_dataset[idx][\"mask\"])\n",
    "print(f\"IoU: {calculateIoU(example_ground_mask, medsam_seg)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### IOU Over 100 Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "test_size = 100\n",
    "#test_size = len(test_img)\n",
    "\n",
    "# Convert the NumPy arrays to Pillow images and store them in a dictionary\n",
    "test_dataset_dict = {\n",
    "    \"image\": [Image.fromarray((img * 255).astype(np.uint8)) for img in test_img[:test_size]],\n",
    "    \"mask\": [Image.fromarray(mask) for mask in test_mask[:test_size]],\n",
    "}\n",
    "\n",
    "# Create the dataset using the datasets.Dataset class\n",
    "test_dataset = Dataset.from_dict(test_dataset_dict)\n",
    "\n",
    "test_ious = []\n",
    "model.to(device)\n",
    "for idx, sample in enumerate(test_dataset):\n",
    "    # Get Image and ground truth mask\n",
    "    image = sample[\"image\"]\n",
    "    ground_truth_mask = np.array(sample[\"mask\"])\n",
    "    \n",
    "    # get box prompt based on ground truth segmentation map\n",
    "    prompt = get_bounding_box(ground_truth_mask)\n",
    "    \n",
    "    # prepare image + box prompt for the model\n",
    "    inputs = processor(image, input_boxes=[[prompt]], return_tensors=\"pt\").to(device)\n",
    "    #inputs = {k:v.squeeze(0) for k,v in inputs.items()}\n",
    "    \n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "      outputs = model(**inputs, multimask_output=False)\n",
    "    # apply sigmoid\n",
    "    medsam_seg_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
    "    # convert soft mask to hard mask\n",
    "    medsam_seg_prob = medsam_seg_prob.cpu().numpy().squeeze()\n",
    "    medsam_seg = (medsam_seg_prob > 0.5).astype(np.uint8)\n",
    "\n",
    "    iou = calculateIoU(ground_truth_mask, medsam_seg)\n",
    "    print(f\"Sample {idx} IoU: {iou}\")\n",
    "    test_ious.append(iou)\n",
    "\n",
    "    \n",
    "print(f\"Average IoUs over {test_size} test sample: {mean(test_ious)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  IOU Over All Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "#test_size = 100\n",
    "test_size = len(test_img)\n",
    "\n",
    "# Convert the NumPy arrays to Pillow images and store them in a dictionary\n",
    "test_dataset_dict = {\n",
    "    \"image\": [Image.fromarray((img * 255).astype(np.uint8)) for img in test_img[:test_size]],\n",
    "    \"mask\": [Image.fromarray(mask) for mask in test_mask[:test_size]],\n",
    "}\n",
    "\n",
    "# Create the dataset using the datasets.Dataset class\n",
    "test_dataset = Dataset.from_dict(test_dataset_dict)\n",
    "\n",
    "test_ious = []\n",
    "model.to(device)\n",
    "for idx, sample in enumerate(test_dataset):\n",
    "    # Get Image and ground truth mask\n",
    "    image = sample[\"image\"]\n",
    "    ground_truth_mask = np.array(sample[\"mask\"])\n",
    "    \n",
    "    # get box prompt based on ground truth segmentation map\n",
    "    prompt = get_bounding_box(ground_truth_mask)\n",
    "    \n",
    "    # prepare image + box prompt for the model\n",
    "    inputs = processor(image, input_boxes=[[prompt]], return_tensors=\"pt\").to(device)\n",
    "    #inputs = {k:v.squeeze(0) for k,v in inputs.items()}\n",
    "    \n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "      outputs = model(**inputs, multimask_output=False)\n",
    "    # apply sigmoid\n",
    "    medsam_seg_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
    "    # convert soft mask to hard mask\n",
    "    medsam_seg_prob = medsam_seg_prob.cpu().numpy().squeeze()\n",
    "    medsam_seg = (medsam_seg_prob > 0.5).astype(np.uint8)\n",
    "\n",
    "    iou = calculateIoU(ground_truth_mask, medsam_seg)\n",
    "    print(f\"Sample {idx} IoU: {iou}\")\n",
    "    test_ious.append(iou)\n",
    "\n",
    "    \n",
    "print(f\"Average IoUs over {test_size} test sample: {mean(test_ious)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 1**\n",
    "- Train Size = ALL\n",
    "- Rank = **100**\n",
    "- Train Epochs = **10**\n",
    "- Test IOU (Over 100 samples)= **0.34**\n",
    "- Test IOU (Over all Test Data): **0.29729**\n",
    "- **Layers Edited**\n",
    "\n",
    "    - model.mask_decoder.transformer.layers:\n",
    "        - layer.self_attn.q_proj \n",
    "        - layer.self_attn.k_proj \n",
    "        - layer.self_attn.v_proj\n",
    "\n",
    "        - layer.mlp.lin1\n",
    "        - layer.mlp.lin2\n",
    "\n",
    "        - layer.cross_attn_image_to_token.q_proj\n",
    "        - layer.cross_attn_image_to_token.k_proj\n",
    "        - layer.cross_attn_image_to_token.v_proj\n",
    "\n",
    "        - layer.cross_attn_image_to_token.q_proj\n",
    "        - layer.cross_attn_image_to_token.k_proj\n",
    "        - layer.cross_attn_image_to_token.v_proj\n",
    "\n",
    "        - layer.layer_norm1\n",
    "\n",
    "    - for layer in model.mask_decoder.transformer.layers:\n",
    "        - layer.layer_norm1\n",
    "        - layer.layer_norm2\n",
    "        - layer.layer_norm3\n",
    "        - layer.layer_norm4\n",
    "\n",
    "    - model.mask_decoder.transformer.final_attn_token_to_image.q_proj\n",
    "    - model.mask_decoder.transformer.final_attn_token_to_image.k_proj\n",
    "    - model.mask_decoder.transformer.final_attn_token_to_image.v_proj\n",
    "\n",
    "**Model 2**\n",
    "- Train Size = ALL\n",
    "- Rank = **100**\n",
    "- Train Epochs = **15**\n",
    "- Mean loss: **0.739632799216355**\n",
    "- Test IOU (Over 100 samples)= **0.24396120759504994**\n",
    "- Test IOU (Over all Test Data): **0.28765982855840583**\n",
    "- **Layers Edited**: (Same as above)\n",
    "\n",
    "**Model 3**\n",
    "- Train Size = **ALL**\n",
    "- Rank = **250**\n",
    "- Train Epochs = **10**\n",
    "- Mean loss: **0.7362756259945838**\n",
    "- Test IOU (Over 100 samples)= **0.242310915626478**\n",
    "- Test IOU (Over all Test Data): **0.30517595636914663**\n",
    "- **Layers Edited**: (Same as above)\n",
    "\n",
    "**Model 4**\n",
    "- Train Size = **ALL**\n",
    "- Rank = **10**\n",
    "- Train Epochs = **20**\n",
    "- Mean loss: **0.3595123540744573**\n",
    "- Test IOU (Over 100 samples)= **0.5290344924635371**\n",
    "- **Layers Edited**: All Vision Enocder, All Mask Decoder, Normalization Layers were trained as well\n",
    "\n",
    "**Model 5**\n",
    "- Train Size = **ALL**\n",
    "- Rank = **25**\n",
    "- Train Epochs = **20**\n",
    "- Mean loss: **0.2858704915053865**\n",
    "- Test IOU (Over 100 samples)= **0.5933433472033873**\n",
    "- Test IOU (Over all Test Data): **0.6708022050721741**\n",
    "- **Layers Edited**: All Vision Enocder, All Mask Decoder, Normalization Layers were trained as well\n",
    "\n",
    "-----------------------------------------------------------------------------------\n",
    "\n",
    "**Original SAM (W/O LoRA)**\n",
    "- Train Size = **ALL**\n",
    "- Rank = **None**\n",
    "- Train Epochs = **20**\n",
    "- Mean loss: **0.07050003955156871**\n",
    "- Test IOU (Over 100 samples)= **0.7202856028634871**\n",
    "- Test IOU (Over all Test Data): **0.7774607072278246**\n",
    "- **Layers Edited**: None\n",
    "\n",
    "**Model 6**\n",
    "- Train Size = **ALL**\n",
    "- Rank = **10**\n",
    "- Train Epochs = **20**\n",
    "- Mean loss: **0.1657026837598934**\n",
    "- Test IOU (Over 100 samples)= **0.6702685182673397**\n",
    "- Test IOU (Over all Test Data): **0.7443639897938635**\n",
    "- **Layers Edited**: QKV layers from Vision Enocder and Mask Decoder\n",
    "\n",
    "**Model 7**\n",
    "- Train Size = **ALL**\n",
    "- Rank = **10**\n",
    "- Train Epochs = **20**\n",
    "- Mean loss: **0.2594345606448535**\n",
    "- Test IOU (Over 100 samples)= **0.6253727173045005**\n",
    "- Test IOU (Over all Test Data): **0.6948658898529892**\n",
    "- **Layers Edited**: QKV layers and MLP layers from Vision Enocder and Mask Decoder\n",
    "\n",
    "**Model 8**\n",
    "- Train Size = **ALL**\n",
    "- Rank = **10**\n",
    "- Train Epochs = **20**\n",
    "- Mean loss: **0.25178625939057847**\n",
    "- Test IOU (Over 100 samples)= **0.6302336585579551**\n",
    "- Test IOU (Over all Test Data): **0.6881568610129997**\n",
    "- **Layers Edited**: QKV and MLP layers + layer norms from Vision Enocder and Mask Decoder\n",
    "\n",
    "**Model 9**\n",
    "- Train Size = **ALL**\n",
    "- Rank = **10**\n",
    "- Train Epochs = **20**\n",
    "- Mean loss: **0.2869026016620171**\n",
    "- Test IOU (Over 100 samples)= **0.5716214652046244**\n",
    "- Test IOU (Over all Test Data): **0.652137245664665**\n",
    "- **Layers Edited**: QKV and MLP layers (including out_proj layers) + layer norms from Vision Enocder and Mask Decoder\n",
    "\n",
    "\n",
    "**Model 10**\n",
    "- Train Size = **ALL**\n",
    "- Rank = **10**\n",
    "- Train Epochs = **20**\n",
    "- Mean loss: **0.31074020895442805**\n",
    "- Test IOU (Over 100 samples)= **0.5697023371616273**\n",
    "- Test IOU (Over all Test Data): **0.650287439480315**\n",
    "- **Layers Edited**: All layers\n",
    "\n",
    "-------------------------\n",
    "**Adjusting Ranks**\n",
    "\n",
    "Baseline\n",
    "- Mean loss: **0.07050003955156871**\n",
    "- Test IOU (Over 100 samples)= **0.7202856028634871**\n",
    "- Test IOU (Over all Test Data): **0.7774607072278246**\n",
    "\n",
    "**Model 11**\n",
    "- Train Size = **ALL**\n",
    "- Rank = **20**\n",
    "- Train Epochs = **20**\n",
    "- Mean loss: **0.15409883258516568**\n",
    "- Test IOU (Over 100 samples)= **0.706868248336556**\n",
    "- Test IOU (Over all Test Data): **0.7615486503716627**\n",
    "- **Layers Edited**: Only self attention blocks in both vision encoder and mask decoder\n",
    "\n",
    "**Model 12**\n",
    "- Train Size = **ALL**\n",
    "- Rank = **100**\n",
    "- Train Epochs = **20**\n",
    "- Mean loss: **0.1575793492199325**\n",
    "- Test IOU (Over 100 samples)= **0.699975307532582**\n",
    "- Test IOU (Over all Test Data): **0.755494044484111**\n",
    "- **Layers Edited**: Only self attention blocks in both vision encoder and mask decoder\n",
    "\n",
    "**Model 13**\n",
    "- Train Size = **ALL**\n",
    "- Rank = **3**\n",
    "- Train Epochs = **20**\n",
    "- Mean loss: **0.15462210127161624**\n",
    "- Test IOU (Over 100 samples)= **0.7169591991935994**\n",
    "- Test IOU (Over all Test Data): **0.7613847020166868**\n",
    "- **Layers Edited**: Only self attention blocks in both vision encoder and mask decoder\n",
    "\n",
    "**Model 14**\n",
    "- Train Size = **ALL**\n",
    "- Rank = **2**\n",
    "- Train Epochs = **20**\n",
    "- Mean loss: **0.15569957190060563**\n",
    "- Test IOU (Over 100 samples)= **0.6973466387152136**\n",
    "- Test IOU (Over all Test Data): **0.7612551539754306**\n",
    "- **Layers Edited**: Only self attention blocks in both vision encoder and mask decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/MathieuNlp/Sam_LoRA?tab=readme-ov-file\n",
    "\n",
    "https://github.com/NielsRogge/Transformers-Tutorials/tree/master/SAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Next Steps**\n",
    "\n",
    "1) Change up the trainable parameter\n",
    "\n",
    "    - **Training all bias vectors associated with modules we apply LoRA to**\n",
    "\n",
    "            lora.mark_only_lora_as_trainable(model, bias='lora_only')\n",
    "\n",
    "    - **Alternatively, we can train *all* bias vectors in the model, including LayerNorm biases**\n",
    "\n",
    "            lora.mark_only_lora_as_trainable(model, bias='all')\n",
    "\n",
    "2) Only use LoRA on q & v projection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
