{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models \n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNCOMMENT IF YOU NEED TO DOWNLOAD DATASET------------------------------------------------------------------------------------------------------\n",
    "# #Download Dataset\n",
    "# import opendatasets as od\n",
    "\n",
    "# od.download(\"https://www.kaggle.com/datasets/aysendegerli/qatacov19-dataset/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "\n",
    "train_img_path = \"/home/cahsi/Josh/Research/venv/Semantic_Segmentation_Research/Dataset/qatacov19-dataset/QaTa-COV19/QaTa-COV19-v2/Train Set/Images/*.png\"\n",
    "test_img_path = \"/home/cahsi/Josh/Research/venv/Semantic_Segmentation_Research/Dataset/qatacov19-dataset/QaTa-COV19/QaTa-COV19-v2/Test Set/Images/*.png\"\n",
    "\n",
    "train_mask_path = \"/home/cahsi/Josh/Research/venv/Semantic_Segmentation_Research/Dataset/qatacov19-dataset/QaTa-COV19/QaTa-COV19-v2/Train Set/Ground-truths/*.png\"\n",
    "test_mask_path = \"/home/cahsi/Josh/Research/venv/Semantic_Segmentation_Research/Dataset/qatacov19-dataset/QaTa-COV19/QaTa-COV19-v2/Test Set/Ground-truths/*.png\"\n",
    "\n",
    "# Load all images in the current folder that end with .png\n",
    "train_img = io.imread_collection(train_img_path)\n",
    "test_img = io.imread_collection(test_img_path)\n",
    "train_mask = io.imread_collection(train_mask_path)\n",
    "test_mask = io.imread_collection(test_mask_path)\n",
    "\n",
    "#Images Info\n",
    "print(\"Length of training raw images: \" + str(len(train_img)) + \"      Shape of an training raw image: \" + str(train_img[0].shape))\n",
    "print(\"Length of training mask images: \" + str(len(train_mask)) + \"     Shape of an training mask image: \" + str(train_mask[0].shape))\n",
    "\n",
    "print(\"\\nLength of test raw images: \" + str(len(test_img)) + \"       Shape of an test raw image: \" + str(test_img[0].shape))\n",
    "print(\"Length of test mask images: \" + str(len(test_mask)) + \"      Shape of an test mask image: \" + str(test_mask[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resizing Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "import cv2\n",
    "\n",
    "def resize_images(images, mask):\n",
    "    output = []\n",
    "    if mask:\n",
    "        for mask in images:\n",
    "            # Perform resizing with nearest neighbor interpolation to maintain binary values\n",
    "            resized_mask = (resize(mask, (256, 256), order=0, anti_aliasing=False) > 0.5).astype(np.uint8)\n",
    "            output.append(resized_mask)\n",
    "    else:\n",
    "        for image in images:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "            resized_image = resize(image, (256, 256), anti_aliasing=False)\n",
    "            output.append(resized_image)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Resize training images to 256x256\n",
    "train_images = np.array(resize_images(train_img, False))\n",
    "\n",
    "# Resize testing images to 256x256\n",
    "test_img = np.array(resize_images(test_img, False))\n",
    "\n",
    "# Resize training masks to 256x256\n",
    "train_masks = np.array(resize_images(train_mask, True))\n",
    "\n",
    "# Resize testing masks to 256x256\n",
    "test_mask = np.array(resize_images(test_mask, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print Shape of resized images\n",
    "print(\"Shape of resized training raw image: \" + str(train_images.shape))\n",
    "print(\"Shape of resized training mask image: \" + str(train_masks.shape))\n",
    "print(\"Shape of resized testing raw image: \" + str(test_img.shape))\n",
    "print(\"Shape of resized testing mask image: \" + str(test_mask.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Subset of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Convert to subset of data\n",
    "subset_size = 0.03\n",
    "\n",
    "#------------------>>>>> Comment if you want to use the full dataset for training\n",
    "# train_images, _, train_masks, _ = train_test_split(train_images, train_masks, train_size=subset_size, random_state=25)\n",
    "\n",
    "images = train_images\n",
    "masks = train_masks\n",
    "\n",
    "#Print Shape of resized images\n",
    "print(\"Shape of resized training raw image: \" + str(images.shape))\n",
    "print(\"Shape of resized training mask image: \" + str(masks.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "# Convert the NumPy arrays to Pillow images and store them in a dictionary\n",
    "dataset_dict = {\n",
    "    \"image\": [Image.fromarray((img * 255).astype(np.uint8)) for img in images],\n",
    "    \"mask\": [Image.fromarray(mask) for mask in masks],\n",
    "}\n",
    "\n",
    "\n",
    "# Create the dataset using the datasets.Dataset class\n",
    "dataset = Dataset.from_dict(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check dataset info\n",
    "print(\"Dataset Info:\\n\" + str(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get random image from training set and plot\n",
    "img_num = random.randint(0, images.shape[0]-1)\n",
    "example_image = dataset[img_num][\"image\"]\n",
    "example_mask = dataset[img_num][\"mask\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 10))\n",
    "\n",
    "# Plot the first image on the left\n",
    "axes[0].imshow(np.array(example_image), cmap='gray')  # Assuming the first image is grayscale\n",
    "axes[0].set_title(\"Image\")\n",
    "\n",
    "# Plot the second image in the middle\n",
    "axes[1].imshow(np.array(example_mask), cmap='gray')  # Assuming the second image is grayscale\n",
    "axes[1].set_title(\"Ground Truth Mask\")\n",
    "\n",
    "# Plot them overlapped on the right\n",
    "axes[2].imshow(np.array(example_image), cmap='gray')  # Assuming the second image is grayscale\n",
    "show_mask(np.array(example_mask), axes[2])\n",
    "axes[2].set_title(\"Result\")\n",
    "\n",
    "\n",
    "# Hide axis ticks and labels\n",
    "for ax in axes:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "# Display the images side by side\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that gets bounding boxes from masks\n",
    "def get_bounding_box(ground_truth_map):\n",
    "  # get bounding box from mask\n",
    "  y_indices, x_indices = np.where(ground_truth_map > 0)\n",
    "  x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "  y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "  # add perturbation to bounding box coordinates\n",
    "  H, W = ground_truth_map.shape\n",
    "  x_min = max(0, x_min - np.random.randint(0, 20))\n",
    "  x_max = min(W, x_max + np.random.randint(0, 20))\n",
    "  y_min = max(0, y_min - np.random.randint(0, 20))\n",
    "  y_max = min(H, y_max + np.random.randint(0, 20))\n",
    "  bbox = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "  return bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAM Dataset/Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SAMDataset(Dataset):\n",
    "  \"\"\"\n",
    "  This class is used to create a dataset that serves input images and masks.\n",
    "  It takes a dataset and a processor as input and overrides the __len__ and __getitem__ methods of the Dataset class.\n",
    "  \"\"\"\n",
    "  def __init__(self, dataset, processor):\n",
    "    self.dataset = dataset\n",
    "    self.processor = processor\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    item = self.dataset[idx]\n",
    "    image = item[\"image\"]\n",
    "    ground_truth_mask = np.array(item[\"mask\"])\n",
    "\n",
    "    # get bounding box prompt\n",
    "    prompt = get_bounding_box(ground_truth_mask)\n",
    "\n",
    "    # prepare image and prompt for the model\n",
    "    inputs = self.processor(image, input_boxes=[[prompt]], return_tensors=\"pt\")\n",
    "\n",
    "    # remove batch dimension which the processor adds by default\n",
    "    inputs = {k:v.squeeze(0) for k,v in inputs.items()}\n",
    "\n",
    "    # add ground truth segmentation\n",
    "    inputs[\"ground_truth_mask\"] = ground_truth_mask\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the processor\n",
    "from transformers import SamProcessor\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "# Create an instance of the SAMDataset\n",
    "train_dataset = SAMDataset(dataset=dataset, processor=processor)\n",
    "\n",
    "print(\"Training: \\n\")\n",
    "example = train_dataset[0]\n",
    "for k,v in example.items():\n",
    "  print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader instance for the training dataset\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, drop_last=False)\n",
    "\n",
    "print(\"Training: \\n\")\n",
    "train_batch = next(iter(train_dataloader))\n",
    "for k,v in train_batch.items():\n",
    "  print(k,v.shape)\n",
    "\n",
    "print(\"Ground Truth Mask Shape: \" + str(train_batch[\"ground_truth_mask\"].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SAM model\n",
    "from transformers import SamModel\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "# make sure we only compute gradients for vision encoder\n",
    "for name, param in model.named_parameters():\n",
    "  if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from segment_anything import build_sam, SamPredictor\n",
    "# from segment_anything import sam_model_registry\n",
    "\n",
    "# import math\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch import Tensor\n",
    "# from torch.nn.parameter import Parameter\n",
    "# from segment_anything.modeling import Sam\n",
    "# from safetensors import safe_open\n",
    "# from safetensors.torch import save_file\n",
    "\n",
    "\n",
    "# class _LoRA_qkv(nn.Module):\n",
    "#     \"\"\"In Sam it is implemented as\n",
    "#     self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "#     B, N, C = x.shape\n",
    "#     qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "#     q, k, v = qkv.unbind(0)\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         qkv: nn.Module,\n",
    "#         linear_a_q: nn.Module,\n",
    "#         linear_b_q: nn.Module,\n",
    "#         linear_a_v: nn.Module,\n",
    "#         linear_b_v: nn.Module,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.qkv = qkv\n",
    "#         self.linear_a_q = linear_a_q\n",
    "#         self.linear_b_q = linear_b_q\n",
    "#         self.linear_a_v = linear_a_v\n",
    "#         self.linear_b_v = linear_b_v\n",
    "#         self.dim = qkv.in_features\n",
    "#         self.w_identity = torch.eye(qkv.in_features)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         qkv = self.qkv(x)  # B,N,N,3*org_C\n",
    "#         new_q = self.linear_b_q(self.linear_a_q(x))\n",
    "#         new_v = self.linear_b_v(self.linear_a_v(x))\n",
    "#         qkv[:, :, :, : self.dim] += new_q\n",
    "#         qkv[:, :, :, -self.dim :] += new_v\n",
    "#         return qkv\n",
    "\n",
    "# class LoRA_Sam(nn.Module):\n",
    "#     \"\"\"Applies low-rank adaptation to a Sam model's image encoder.\n",
    "\n",
    "#     Args:\n",
    "#         sam_model: a vision transformer model, see base_vit.py\n",
    "#         r: rank of LoRA\n",
    "#         num_classes: how many classes the model output, default to the vit model\n",
    "#         lora_layer: which layer we apply LoRA.\n",
    "\n",
    "#     Examples::\n",
    "#         >>> model = ViT('B_16_imagenet1k')\n",
    "#         >>> lora_model = LoRA_ViT(model, r=4)\n",
    "#         >>> preds = lora_model(img)\n",
    "#         >>> print(preds.shape)\n",
    "#         torch.Size([1, 1000])\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, sam_model: Sam, r: int, lora_layer=None):\n",
    "#         super(LoRA_Sam, self).__init__()\n",
    "\n",
    "#         assert r > 0\n",
    "        \n",
    "#         if lora_layer:\n",
    "#             self.lora_layer = lora_layer\n",
    "#         else:\n",
    "#             self.lora_layer = list(range(len(sam_model.vision_encoder.layers)))\n",
    "#         # create for storage, then we can init them or load weights\n",
    "#         self.w_As = []  # These are linear layers\n",
    "#         self.w_Bs = []\n",
    "\n",
    "#         # lets freeze first\n",
    "#         for param in sam_model.vision_encoder.layers.parameters():\n",
    "#             param.requires_grad = False\n",
    "\n",
    "#         # Here, we do the surgery\n",
    "#         for t_layer_i, blk in enumerate(sam_model.vision_encoder.layers):\n",
    "#             # If we only want few lora layer instead of all\n",
    "#             if t_layer_i not in self.lora_layer:\n",
    "#                 continue\n",
    "#             w_qkv_linear = blk.attn.qkv\n",
    "#             self.dim = w_qkv_linear.in_features\n",
    "#             w_a_linear_q = nn.Linear(self.dim, r, bias=False)\n",
    "#             w_b_linear_q = nn.Linear(r, self.dim, bias=False)\n",
    "#             w_a_linear_v = nn.Linear(self.dim, r, bias=False)\n",
    "#             w_b_linear_v = nn.Linear(r, self.dim, bias=False)\n",
    "#             self.w_As.append(w_a_linear_q)\n",
    "#             self.w_Bs.append(w_b_linear_q)\n",
    "#             self.w_As.append(w_a_linear_v)\n",
    "#             self.w_Bs.append(w_b_linear_v)\n",
    "#             blk.attn.qkv = _LoRA_qkv(\n",
    "#                 w_qkv_linear,\n",
    "#                 w_a_linear_q,\n",
    "#                 w_b_linear_q,\n",
    "#                 w_a_linear_v,\n",
    "#                 w_b_linear_v,\n",
    "#             )\n",
    "#         self.reset_parameters()\n",
    "#         self.sam = sam_model\n",
    "\n",
    "#     def load_fc_parameters(self, filename: str) -> None:\n",
    "#         r\"\"\"Only safetensors is supported now.\n",
    "\n",
    "#         pip install safetensor if you do not have one installed yet.\n",
    "#         \"\"\"\n",
    "\n",
    "#         assert filename.endswith(\".safetensors\")\n",
    "#         _in = self.lora_vit.head.in_features\n",
    "#         _out = self.lora_vit.head.out_features\n",
    "#         with safe_open(filename, framework=\"pt\") as f:\n",
    "#             saved_key = f\"fc_{_in}in_{_out}out\"\n",
    "#             try:\n",
    "#                 saved_tensor = f.get_tensor(saved_key)\n",
    "#                 self.lora_vit.head.weight = Parameter(saved_tensor)\n",
    "#             except ValueError:\n",
    "#                 print(\"this fc weight is not for this model\")\n",
    "\n",
    "#     def save_lora_parameters(self, filename: str) -> None:\n",
    "#         r\"\"\"Only safetensors is supported now.\n",
    "\n",
    "#         pip install safetensor if you do not have one installed yet.\n",
    "        \n",
    "#         save both lora and fc parameters.\n",
    "#         \"\"\"\n",
    "\n",
    "#         assert filename.endswith(\".safetensors\")\n",
    "\n",
    "#         num_layer = len(self.w_As)  # actually, it is half\n",
    "#         a_tensors = {f\"w_a_{i:03d}\": self.w_As[i].weight for i in range(num_layer)}\n",
    "#         b_tensors = {f\"w_b_{i:03d}\": self.w_Bs[i].weight for i in range(num_layer)}\n",
    "        \n",
    "#         merged_dict = {**a_tensors, **b_tensors}\n",
    "#         save_file(merged_dict, filename)\n",
    "\n",
    "#     def load_lora_parameters(self, filename: str) -> None:\n",
    "#         r\"\"\"Only safetensors is supported now.\n",
    "\n",
    "#         pip install safetensor if you do not have one installed yet.\\\n",
    "            \n",
    "#         load both lora and fc parameters.\n",
    "#         \"\"\"\n",
    "\n",
    "#         assert filename.endswith(\".safetensors\")\n",
    "\n",
    "#         with safe_open(filename, framework=\"pt\") as f:\n",
    "#             for i, w_A_linear in enumerate(self.w_As):\n",
    "#                 saved_key = f\"w_a_{i:03d}\"\n",
    "#                 saved_tensor = f.get_tensor(saved_key)\n",
    "#                 w_A_linear.weight = Parameter(saved_tensor)\n",
    "\n",
    "#             for i, w_B_linear in enumerate(self.w_Bs):\n",
    "#                 saved_key = f\"w_b_{i:03d}\"\n",
    "#                 saved_tensor = f.get_tensor(saved_key)\n",
    "#                 w_B_linear.weight = Parameter(saved_tensor)\n",
    "                \n",
    "#     def reset_parameters(self) -> None:\n",
    "#         for w_A in self.w_As:\n",
    "#             nn.init.kaiming_uniform_(w_A.weight, a=math.sqrt(5))\n",
    "#         for w_B in self.w_Bs:\n",
    "#             nn.init.zeros_(w_B.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"SAM total params: {sam_total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.mask_decoder.transformer.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loralib as lora\n",
    "\n",
    "rank = 20\n",
    "\n",
    "for layer in model.mask_decoder.transformer.layers:\n",
    "    \n",
    "    #Self attention block\n",
    "    layer.self_attn.q_proj = lora.Linear(256, 256, r=rank)\n",
    "    layer.self_attn.k_proj = lora.Linear(256, 256, r=rank)\n",
    "    layer.self_attn.v_proj = lora.Linear(256, 256, r=rank)\n",
    "\n",
    "    #MLP block\n",
    "    layer.mlp.lin1 = lora.Linear(256, 2048, r=rank)\n",
    "    layer.mlp.lin2 = lora.Linear(2048, 256, r=rank)\n",
    "    \n",
    "    #Cross attention block (Token -> Image)\n",
    "    layer.cross_attn_image_to_token.q_proj = lora.Linear(256, 128, r=rank)\n",
    "    layer.cross_attn_image_to_token.k_proj = lora.Linear(256, 128, r=rank)\n",
    "    layer.cross_attn_image_to_token.v_proj = lora.Linear(256, 128, r=rank)\n",
    "\n",
    "\n",
    "    #Cross attention block (Image -> Token)\n",
    "    layer.cross_attn_image_to_token.q_proj = lora.Linear(256, 128, r=rank)\n",
    "    layer.cross_attn_image_to_token.k_proj = lora.Linear(256, 128, r=rank)\n",
    "    layer.cross_attn_image_to_token.v_proj = lora.Linear(256, 128, r=rank)\n",
    "\n",
    "\n",
    "model.mask_decoder.transformer.final_attn_token_to_image.q_proj = lora.Linear(256, 128, r=rank)\n",
    "model.mask_decoder.transformer.final_attn_token_to_image.k_proj = lora.Linear(256, 128, r=rank)\n",
    "model.mask_decoder.transformer.final_attn_token_to_image.v_proj = lora.Linear(256, 128, r=rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"LoRA-SAM total params: {sam_total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import monai\n",
    "\n",
    "# Initialize the optimizer and the loss function\n",
    "optimizer = Adam(model.mask_decoder.parameters(), lr=1e-5, weight_decay=0)\n",
    "\n",
    "#Try DiceFocalLoss, FocalLoss, DiceCELoss\n",
    "seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DataParallel\n",
    "from torch.nn.functional import threshold, normalize\n",
    "\n",
    "#Training loop\n",
    "num_epochs = 7\n",
    "mean_epoch_loss = []\n",
    "\n",
    "# This sets requires_grad to False for all parameters without the string \"lora_\" in their names\n",
    "lora.mark_only_lora_as_trainable(model)\n",
    "\n",
    "#Set layer-norm to be trainable\n",
    "for layer in model.mask_decoder.transformer.layers:\n",
    "    layer.layer_norm1.requires_grad_(True)\n",
    "    layer.layer_norm2.requires_grad_(True)\n",
    "    layer.layer_norm3.requires_grad_(True)\n",
    "    layer.layer_norm4.requires_grad_(True)\n",
    "\n",
    "\n",
    "# Move your model to GPU devices\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = model.to(device)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # forward pass\n",
    "        outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
    "                        input_boxes=batch[\"input_boxes\"].to(device),\n",
    "                        multimask_output=False)\n",
    "\n",
    "        # compute loss\n",
    "        predicted_masks = outputs.pred_masks.squeeze(1)\n",
    "        ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
    "        loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))\n",
    "\n",
    "        # backward pass (compute gradients of parameters w.r.t. loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "    print(f'EPOCH: {epoch}')\n",
    "    print(f'Mean loss: {mean(epoch_losses)}')\n",
    "    mean_epoch_loss.append(mean_epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state dictionary to a file\n",
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': mean(epoch_losses),\n",
    "            }, f\"/home/cahsi/Josh/Research/venv/Semantic_Segmentation_Research/LoRA_SAM/LoRA_sam_model_checkpoint_rank{rank}.pth\")\n",
    "\n",
    "# Save the LoRA parameters of the model\n",
    "torch.save(lora.lora_state_dict(model), f\"/home/cahsi/Josh/Research/venv/Semantic_Segmentation_Research/LoRA_SAM/lora_rank{rank}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SamModel, SamConfig, SamProcessor\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model configuration\n",
    "model_config = SamConfig.from_pretrained(\"facebook/sam-vit-base\")\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "# Create an instance of the model architecture with the loaded configuration\n",
    "lora_model = SamModel(config=model_config)\n",
    "\n",
    "#Load pretrained model\n",
    "lora_model.load_state_dict(torch.load(f\"/home/cahsi/Josh/Research/venv/Semantic_Segmentation_Research/LoRA_SAM/LoRA_sam_model_checkpoint_rank{rank}.pth\"), strict=False)\n",
    "\n",
    "#Load LoRA checkpoint\n",
    "lora_model.load_state_dict(torch.load(f\"/home/cahsi/Josh/Research/venv/Semantic_Segmentation_Research/LoRA_SAM/lora_rank{rank}.pt\"), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the device to cuda if available, otherwise use cpu\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lora_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# let's take a random training example\n",
    "idx = random.randint(0, images.shape[0]-1)\n",
    "\n",
    "# load image\n",
    "test_image = dataset[idx][\"image\"]\n",
    "test_image = np.array(test_image.convert(\"RGB\"))\n",
    "\n",
    "# get box prompt based on ground truth segmentation map\n",
    "ground_truth_mask = np.array(dataset[idx][\"mask\"])\n",
    "prompt = get_bounding_box(ground_truth_mask)\n",
    "\n",
    "# prepare image + box prompt for the model\n",
    "inputs = processor(test_image, input_boxes=[[prompt]], return_tensors=\"pt\")\n",
    "\n",
    "# Move the input tensor to the GPU if it's not already there\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "lora_model.eval()\n",
    "\n",
    "# forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = lora_model(**inputs, multimask_output=False)\n",
    "\n",
    "# apply sigmoid\n",
    "medsam_seg_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
    "# convert soft mask to hard mask\n",
    "medsam_seg_prob = medsam_seg_prob.cpu().numpy().squeeze()\n",
    "medsam_seg = (medsam_seg_prob > 0.5).astype(np.uint8)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 10))\n",
    "\n",
    "# Plot the first image on the left\n",
    "axes[0].imshow(test_image, cmap='gray')  # Assuming the first image is grayscale\n",
    "show_mask(np.array(ground_truth_mask), axes[0])\n",
    "axes[0].set_title(\"Ground Truth Mask\")\n",
    "\n",
    "# Plot the second image on the right\n",
    "axes[1].imshow(test_image, cmap='gray')  # Assuming the second image is grayscale\n",
    "show_mask(np.array(medsam_seg), axes[1])\n",
    "axes[1].set_title(\"Predicted Mask\")\n",
    "\n",
    "# Hide axis ticks and labels\n",
    "for ax in axes:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "# Display the images side by side\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateIoU(ground_mask, pred_mask):\n",
    "        # Calculate the TP, FP, FN\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        FN = 0\n",
    " \n",
    "        for i in range(len(ground_mask)):\n",
    "            for j in range(len(ground_mask[0])):\n",
    "                if ground_mask[i][j] == 1 and pred_mask[i][j] == 1:\n",
    "                    TP += 1\n",
    "                elif ground_mask[i][j] == 0 and pred_mask[i][j] == 1:\n",
    "                    FP += 1\n",
    "                elif ground_mask[i][j] == 1 and pred_mask[i][j] == 0:\n",
    "                    FN += 1\n",
    " \n",
    "        # Calculate IoU\n",
    "        iou = TP / (TP + FP + FN)\n",
    " \n",
    "        return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_ground_mask = np.array(dataset[idx][\"mask\"])\n",
    "print(f\"IoU: {calculateIoU(example_ground_mask, medsam_seg)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "# Convert the NumPy arrays to Pillow images and store them in a dictionary\n",
    "test_dataset_dict = {\n",
    "    \"image\": [Image.fromarray((img * 255).astype(np.uint8)) for img in test_img[0:100]],\n",
    "    \"mask\": [Image.fromarray(mask) for mask in test_mask[0:100]],\n",
    "}\n",
    "\n",
    "# Create the dataset using the datasets.Dataset class\n",
    "test_dataset = Dataset.from_dict(test_dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ious = []\n",
    "model.to(device)\n",
    "for idx, sample in enumerate(test_dataset):\n",
    "    # Get Image and ground truth mask\n",
    "    image = sample[\"image\"]\n",
    "    ground_truth_mask = np.array(sample[\"mask\"])\n",
    "    \n",
    "    # get box prompt based on ground truth segmentation map\n",
    "    prompt = get_bounding_box(ground_truth_mask)\n",
    "    \n",
    "    # prepare image + box prompt for the model\n",
    "    inputs = processor(image, input_boxes=[[prompt]], return_tensors=\"pt\").to(device)\n",
    "    #inputs = {k:v.squeeze(0) for k,v in inputs.items()}\n",
    "    \n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "      outputs = model(**inputs, multimask_output=False)\n",
    "    # apply sigmoid\n",
    "    medsam_seg_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
    "    # convert soft mask to hard mask\n",
    "    medsam_seg_prob = medsam_seg_prob.cpu().numpy().squeeze()\n",
    "    medsam_seg = (medsam_seg_prob > 0.5).astype(np.uint8)\n",
    "\n",
    "    iou = calculateIoU(ground_truth_mask, medsam_seg)\n",
    "    print(f\"Sample {idx} IoU: {iou}\")\n",
    "    test_ious.append(iou)\n",
    "\n",
    "    \n",
    "print(f\"Average IoUs over 100 test sample: {mean(test_ious)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 1**\n",
    "- Train Size = ALL\n",
    "- Rank = 20\n",
    "- Train Epochs = 7\n",
    "- Test IOU = 0.32\n",
    "- **Layers Edited**\n",
    "\n",
    "    - model.mask_decoder.transformer.layers:\n",
    "        - layer.self_attn.q_proj \n",
    "        - layer.self_attn.k_proj \n",
    "        - layer.self_attn.v_proj\n",
    "\n",
    "        - layer.mlp.lin1\n",
    "        - layer.mlp.lin2\n",
    "\n",
    "        - layer.cross_attn_image_to_token.q_proj\n",
    "        - layer.cross_attn_image_to_token.k_proj\n",
    "        - layer.cross_attn_image_to_token.v_proj\n",
    "\n",
    "        - layer.cross_attn_image_to_token.q_proj\n",
    "        - layer.cross_attn_image_to_token.k_proj\n",
    "        - layer.cross_attn_image_to_token.v_proj\n",
    "\n",
    "        - layer.layer_norm1.requires_grad_(True)\n",
    "\n",
    "    - for layer in model.mask_decoder.transformer.layers:\n",
    "        - layer.layer_norm1.requires_grad_(True)\n",
    "        - layer.layer_norm2.requires_grad_(True)\n",
    "        - layer.layer_norm3.requires_grad_(True)\n",
    "        - layer.layer_norm4.requires_grad_(True)\n",
    "\n",
    "    - model.mask_decoder.transformer.final_attn_token_to_image.q_proj\n",
    "    - model.mask_decoder.transformer.final_attn_token_to_image.k_proj\n",
    "    - model.mask_decoder.transformer.final_attn_token_to_image.v_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/MathieuNlp/Sam_LoRA?tab=readme-ov-file\n",
    "\n",
    "https://github.com/NielsRogge/Transformers-Tutorials/tree/master/SAM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
